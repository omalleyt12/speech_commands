{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.platform import gfile\n",
    "from tensorflow.python.ops import io_ops\n",
    "from tensorflow.contrib.framework.python.ops import audio_ops as contrib_audio\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "training_step_list = [1,1]\n",
    "learning_rate_list = [0.001,0.0001]\n",
    "\n",
    "data_dir = \"train/audio\"\n",
    "summary_dir = \"logs\" # where to save summary logs for Tensorboard\n",
    "wanted_words = [\"silence\",\"unknown\",\"yes\",\"no\",\"up\",\"down\",\"left\",\"right\",\"on\",\"off\",\"stop\",\"go\",\"silence\"]\n",
    "\n",
    "sample_rate = 16000 # per sec\n",
    "clip_duration_ms = 1000 #\n",
    "\n",
    "max_background_volume = 0.1 # how loud background noise should be, [0,1]\n",
    "background_frequency = 0.8 # how many of training samps have noise added\n",
    "\n",
    "silence_percentage = 10.0 # what percent of training data should be silence\n",
    "unknown_percentage = 10.0 # what percent of training data should be unknown words\n",
    "\n",
    "max_time_shift_ms = 100.0 # max range to randomly shift the training audio by time\n",
    "\n",
    "window_size_ms = 30.0 # millisec length of frequency analysis window\n",
    "window_stride_ms = 10.0\n",
    "\n",
    "window_size_samples = int(1600 * window_size_ms / 1000)\n",
    "window_stride_samples = int(1600 * window_stride_ms / 1000)\n",
    "\n",
    "dct_coefficient_count = 40 # bins to use for MFCC fingerprint\n",
    "\n",
    "percent_test = 10 # test set\n",
    "percent_val = 10 # val set\n",
    "\n",
    "def prepare_data_index():\n",
    "    random.seed(111)\n",
    "    wanted_words_index = {}\n",
    "    for i, wanted_word in enumerate(wanted_words):\n",
    "        wanted_words_index[wanted_word] = i + 2 # bc silence, unknown will be first\n",
    "\n",
    "    data_index = {\"val\":[],\"test\":[],\"train\":[]}\n",
    "    unknown_index = {\"val\":[],\"test\":[],\"train\":[]}\n",
    "\n",
    "    all_words = {}\n",
    "\n",
    "    for wav_path in gfile.Glob(data_dir + \"/*/*.wav\"):\n",
    "        word = os.path.split(os.path.dirname(wav_path))[-1].lower()\n",
    "        if word == \"_background_noise_\":\n",
    "            continue # don't include yet\n",
    "\n",
    "        all_words[word] = True\n",
    "\n",
    "        set_name = which_set(wav_path)\n",
    "\n",
    "        if word in wanted_words:\n",
    "            data_index[set_name].append({\"label\":word,\"file\":wav_path})\n",
    "        else:\n",
    "            unknown_index[set_name].append({\"label\":word,\"file\":wav_path})\n",
    "\n",
    "    silence_wav_path = data_index[\"train\"][0][\"file\"] # arbitrary, to be used for silence\n",
    "    for set_name in ['val','test','train']:\n",
    "        # add silence to val, train, test\n",
    "        set_size = len(data_index[set_name])\n",
    "        silence_size = int(math.ceil(set_size * silence_percentage) / 100)\n",
    "        for _ in range(silence_size):\n",
    "            data_index[set_name].append({\"label\":\"silence\",\"file\":silence_wav_path})\n",
    "\n",
    "        # add unknown words to val, train, test\n",
    "        random.shuffle(unknown_index[set_name])\n",
    "        unknown_size = int(math.cel(set_size * unknown_percentage) / 100)\n",
    "        data_index[set_name].extend(unknown_index[set_name][:unknown_size])\n",
    "\n",
    "    # randomize each set\n",
    "    for set_name in ['val','test','train']:\n",
    "        random.shuffle(data_index[set_index])\n",
    "\n",
    "    # get word2index mapping\n",
    "    word2index = {}\n",
    "    for word in all_words:\n",
    "        if word in wanted_words_index:\n",
    "            word2index[word] = wanted_words_index[word]\n",
    "        else:\n",
    "            word2index[word] = 1\n",
    "    word2index[\"silence\"] = 0\n",
    "\n",
    "    return data_index, word2index\n",
    "\n",
    "def prepare_test_data():\n",
    "    test_dir = \"test/audio\"\n",
    "    test_index = []\n",
    "    for wav_path in gfile.Glob(test_dir + \"/*.wav\"):\n",
    "        text_index.append(wav_path)\n",
    "    return test_index\n",
    "\n",
    "\n",
    "\n",
    "def which_set(wav_path):\n",
    "    # split into train, test, val sets deterministically\n",
    "    wav_name = re.sub(r'_nohas_.*$','',os.path.basename(wav_path)) # so that all samps from same user are grouped together\n",
    "    hash_name_hashed = hashlib.sha1(compat.as_bytes(hash_name)).hexdigest()\n",
    "    MAX_NUM_WAVS_PER_CLASS = 2**27 - 1\n",
    "    percentage_hash = ((int(hash_name_hashed, 16) %\n",
    "                    (MAX_NUM_WAVS_PER_CLASS + 1)) *\n",
    "                    (100.0 / MAX_NUM_WAVS_PER_CLASS))\n",
    "    if percentage_hash < percent_val:\n",
    "        result = 'val'\n",
    "    elif percentage_hash < (percent_val + percent_test):\n",
    "        result = 'test'\n",
    "    else:\n",
    "        result = 'train'\n",
    "    return result\n",
    "\n",
    "\n",
    "def prepare_background_data():\n",
    "    bg_data = []\n",
    "    bg_path = \"/train/audio/_background_noise_/*.wav\"\n",
    "    with tf.Session(graph=tf.Graph()) as sess:\n",
    "        wav_filename_ph = tf.placeholder(tf.string,[])\n",
    "        wav_loader = io_ops.read_file(wav_filename_ph)\n",
    "        wav_decoder = contrib_audio.decode_wav(wav_loader,desired_channels=1)\n",
    "        for wav_path in gfile.Glob(bg_path):\n",
    "            wav_data = sess.run(wav_decoder,feed_dict={wav_filename_ph:wav_path}).audio.flatten()\n",
    "            bg_data.append(wav_data)\n",
    "    return bg_data\n",
    "\n",
    "def main():\n",
    "\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    data_index, word2index = prepare_data_index()\n",
    "    bg_data = prepare_background_data()\n",
    "\n",
    "    # Preprocessing Graph parts\n",
    "    wav_filename_ph = tf.placeholder(tf.string,[])\n",
    "    wav_loader = io_ops.read_file(wav_filename_ph)\n",
    "    wav_decoder = contrib_audio.decode_wav(wav_loader,desired_channels=1,desired_samples=1600) # length of 1 sec\n",
    "\n",
    "    volume_ph = tf.placeholder(tf.float32,[])\n",
    "\n",
    "    # this seems weird, shouldn't it do some kind of conditional scaling??\n",
    "    # nah, we're doing spectrograms bruh, plus this is just to zero out the volume for silence\n",
    "    scaled_wav = tf.multiply(wav_decoder.audio, volume_ph)\n",
    "\n",
    "    time_shift_padding_ph = tf.placeholder(tf.int32,[2,2])\n",
    "    time_shift_offset_ph = tf.placeholder(tf.int32,[2])\n",
    "\n",
    "    padded_wav = tf.pad(scaled_wav,time_shift_padding_ph,mode=\"CONSTANT\")\n",
    "    sliced_wav = tf.slice(padded_wav,time_shift_offset_ph,[1600,-1])\n",
    "\n",
    "    bg_placeholder = tf.placeholder(tf.float32,[1600,1])\n",
    "    bg_volume_ph = tf.placeholder(tf.float32,[])\n",
    "\n",
    "    scaled_bg = tf.multiply(bg_placeholder,bg_volume_ph)\n",
    "\n",
    "    wav_with_bg = tf.add(sliced_wav,scaled_bg)\n",
    "\n",
    "    clamped_wav = tf.clip_by_value(wav_with_bg,-1.0,1.0)\n",
    "\n",
    "    spectrogram = contrib_audio.audio_spectrogram(\n",
    "        clamped_wav,\n",
    "        window_size = window_size_samples,\n",
    "        stride = window_stride_samples,\n",
    "        magnitude_squared = True\n",
    "    )\n",
    "\n",
    "    mfcc = contrib_audio.mfcc(\n",
    "        spectrogram,\n",
    "        wav_decoder.sample_rate,\n",
    "        dct_coefficient_count = dct_coefficient_count\n",
    "    )\n",
    "\n",
    "    mfcc_height = mfcc.shape[1]\n",
    "    mfcc_width = mfcc.shape[2]\n",
    "\n",
    "    fingerprint_ph = tf.placeholder(tf.float32,[None,mfcc_height,mfcc_width])\n",
    "\n",
    "\n",
    "\n",
    "    for steps, lrate in zip(training_step_list,learning_rate_list):\n",
    "        for i in range(steps):\n",
    "            data = np.zeros((batch_size,mfcc_height,mfcc_width))\n",
    "            labels = np.zeros((batch_size,len(wanted_words)))\n",
    "            for j in batch_size:\n",
    "                samp_index = np.random.randint(len(data_index[\"train\"]))\n",
    "                samp_data = data_index[\"train\"][samp_index]\n",
    "\n",
    "                time_shift_amount = np.random.randint(-max_time_shift_ms,max_time_shift_ms)\n",
    "                if time_shift_amount > 0:\n",
    "                    time_shift_padding = [[time_shift_amount,0],[0,0]]\n",
    "                    time_shift_offset = [0,0]\n",
    "                else:\n",
    "                    time_shift_padding = [[0,-time_shift_amount],[0,0]]\n",
    "                    time_shift_offset = [-time_shift_amount,0]\n",
    "\n",
    "                bg_index = np.random.randint(len(bg_data))\n",
    "                bg_samp = bg_data[bg_index]\n",
    "                bg_offset = np.random.randint(0,len(bg_samp) - 1600)\n",
    "                bg_sliced = bg_samp[bg_offset:(bg_offset + 1600)]\n",
    "                bg_sliced = bg_sliced.reshape(1600,-1)\n",
    "                if np.random.uniform(0,1) < background_frequency:\n",
    "                    bg_volume = np.random.uniform(0,max_background_volume)\n",
    "                else:\n",
    "                    bg_volume = 0\n",
    "\n",
    "                if samp_data[\"label\"] == \"silence\":\n",
    "                    volume = 0 # zero out the foreground for the silence labels\n",
    "                else:\n",
    "                    volume = 1\n",
    "\n",
    "                data[j,:] = sess.run(mfcc,feed_dict={\n",
    "                    wav_filename_ph: samp_data[\"file\"],\n",
    "                    volume_ph: volume,\n",
    "                    time_shift_padding_ph: time_shift_padding,\n",
    "                    time_shift_offset_ph: time_shift_offset,\n",
    "                    background_ph: bg_sliced,\n",
    "                    bg_volume_ph: bg_volume\n",
    "                }).flatten()\n",
    "                labels[j,word2index[samp_data[\"label\"]]] = 1\n",
    "                print(data)\n",
    "\n",
    "\n",
    "            # now here's where we run the real, convnet part\n",
    "#             stuff = sess.run([bunch_of_stuff],feed_dict={\n",
    "\n",
    "#             })\n",
    "\n",
    "    # now here's where we run the test classification\n",
    "    test_index = prepare_test_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-6137cde4893c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
