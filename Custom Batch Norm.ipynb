{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bn_layer(x, scope, is_training, epsilon=0.001, decay=0.99, reuse=None):\n",
    "    \"\"\"\n",
    "    Performs a batch normalization layer\n",
    "\n",
    "    Args:\n",
    "        x: input tensor\n",
    "        scope: scope name\n",
    "        is_training: python boolean value\n",
    "        epsilon: the variance epsilon - a small float number to avoid dividing by 0\n",
    "        decay: the moving average decay\n",
    "\n",
    "    Returns:\n",
    "        The ops of a batch normalization layer\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        shape = x.get_shape().as_list()\n",
    "        # gamma: a trainable scale factor\n",
    "        gamma = tf.get_variable(\"gamma\", shape[-1], initializer=tf.constant_initializer(1.0), trainable=True)\n",
    "        # beta: a trainable shift value\n",
    "        beta = tf.get_variable(\"beta\", shape[-1], initializer=tf.constant_initializer(0.0), trainable=True)\n",
    "        moving_avg = tf.get_variable(\"moving_avg\", shape[-1], initializer=tf.constant_initializer(0.0), trainable=False)\n",
    "        moving_var = tf.get_variable(\"moving_var\", shape[-1], initializer=tf.constant_initializer(1.0), trainable=False)\n",
    "        if is_training:\n",
    "            # tf.nn.moments == Calculate the mean and the variance of the tensor x\n",
    "            avg, var = tf.nn.moments(x, np.arange(len(shape)-1), keep_dims=True)\n",
    "            avg=tf.reshape(avg, [avg.shape.as_list()[-1]])\n",
    "            var=tf.reshape(var, [var.shape.as_list()[-1]])\n",
    "            #update_moving_avg = moving_averages.assign_moving_average(moving_avg, avg, decay)\n",
    "            update_moving_avg=tf.assign(moving_avg, moving_avg*decay+avg*(1-decay))\n",
    "            #update_moving_var = moving_averages.assign_moving_average(moving_var, var, decay)\n",
    "            update_moving_var=tf.assign(moving_var, moving_var*decay+var*(1-decay))\n",
    "            control_inputs = [update_moving_avg, update_moving_var]\n",
    "        else:\n",
    "            avg = moving_avg\n",
    "            var = moving_var\n",
    "            control_inputs = []\n",
    "        with tf.control_dependencies(control_inputs):\n",
    "            output = tf.nn.batch_normalization(x, avg, var, offset=beta, scale=gamma, variance_epsilon=epsilon)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def bn_layer_top(x, scope, is_training, epsilon=0.001, decay=0.99):\n",
    "    \"\"\"\n",
    "    Returns a batch normalization layer that automatically switch between train and test phases based on the \n",
    "    tensor is_training\n",
    "\n",
    "    Args:\n",
    "        x: input tensor\n",
    "        scope: scope name\n",
    "        is_training: boolean tensor or variable\n",
    "        epsilon: epsilon parameter - see batch_norm_layer\n",
    "        decay: epsilon parameter - see batch_norm_layer\n",
    "\n",
    "    Returns:\n",
    "        The correct batch normalization layer based on the value of is_training\n",
    "    \"\"\"\n",
    "    #assert isinstance(is_training, (ops.Tensor, variables.Variable)) and is_training.dtype == tf.bool\n",
    "\n",
    "    return tf.cond(\n",
    "        is_training,\n",
    "        lambda: bn_layer(x=x, scope=scope, epsilon=epsilon, decay=decay, is_training=True, reuse=None),\n",
    "        lambda: bn_layer(x=x, scope=scope, epsilon=epsilon, decay=decay, is_training=False, reuse=True),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_ph = tf.placeholder(tf.float32,[None,10])\n",
    "is_training_ph = tf.placeholder(tf.bool)\n",
    "wav_norm = bn_layer_top(wav_ph,\"my_thing\",is_training_ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavs = np.stack([np.ones((10),np.float32),np.zeros((10),np.float32),np.arange(10,dtype=np.float32)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7.9472864e-09"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(wav_norm,{is_training_ph:True,wav_ph:wavs}).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda 3.6",
   "language": "python",
   "name": "ana36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
