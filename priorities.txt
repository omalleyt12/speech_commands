I have this idea about how to bootstrap using learning on the test data:

So, first train on the training data only

Then, train on the test data + training data as follows:

1) Loss = (prob of test_label - 0.50)*2*softmax_loss(test_pred,test_label)
2) Update the (prob of test_label) based on new softmax calculation

Repeat for many iterations, and this should, over time, correct the incorrect labels on the test data and gradually move to a solution that
generalizes well on the test data

In this way, you get a lot more exposure to data, you get a lot more data augmentation, and the labels that you're not sure about are not used that much
in the loss...until you become more sure about them



Ok, at this point my priorities HAVE to be:

1) Incorporating Heng's strategy for unknowns and silence
2) Creating models for MFCC and raw wave features
	- I will have to spin up 3 servers so that I can run these in parallel
	- I will have to adopt argparsing so that I don't have to keep changing things, and can keep GH up-to-date
3) Ensembling these models together
	- Different models to try:
		1) Try a highly regularized L2 model version
		DONE 1) Models using short STFT window (16ms,8ms stride)
		DONE 1) Models with no noise
		DONE 1) Models using mixup data augmentation
		DONE 1) Models that train on pseudo labels for silence and unknowns
		1) 5 Overdrive Pseudo-Label models with different seeds, since this is my best model
		DONE 2) Ok Conv
		DONE 3) An RNN version of Overdrive
		4) An average-pooled (at all levels) version of Overdrive
		4) An average-pooled specialized Overdrive
		DONE 4) An Overdrive that squeezes down the feature set with extrac convs
		DONE 5) An Overdrive with less parameter
		4) A flat (or locally max-pooled) version of Overdrive
		5) Overdrive models that were trained concurrently with the known unknowns
		6) New Drive
		7) Models that do volume equalization differently (avg vol, peak vol with larger strides)
		DONE 8) DONE - Models that use frame-by-frame standardization
		8) DON'T WANT TO - Heng's MFCC models, 2 of them
		9) Heng's Raw Wave models, 2 of them
		10) DON'T WANT TO - Heng's Log Mel models, 2 of them
		11) The Hello Edge DS-CNN
		DONEISH 12) The Hello Edge R-CNN

4) Different strategies for Ensembling:

DONE, no - Try averaging the squares of predictions
DONE, yes - Try averaging the square roots of predictions
CAN'T DO WITHOUT RETRAINING - Try making a second-layer model that combines the results of all models into one prediction???
	
5) I NEED TO FIGURE OUT WHAT IS SO WRONG WITH THE TEST DATA
		
6) Gated Convnets? Maxout Convnets? Try each on the Pseudo-labelled version
