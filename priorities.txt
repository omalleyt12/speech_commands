I have this idea about how to bootstrap using learning on the test data:

So, first train on the training data only

Then, train on the test data + training data as follows:

1) Loss = (prob of test_label - 0.50)*2*softmax_loss(test_pred,test_label)
2) Update the (prob of test_label) based on new softmax calculation

Repeat for many iterations, and this should, over time, correct the incorrect labels on the test data and gradually move to a solution that
generalizes well on the test data

In this way, you get a lot more exposure to data, you get a lot more data augmentation, and the labels that you're not sure about are not used that much
in the loss...until you become more sure about them



Ok, at this point my priorities HAVE to be:

1) Incorporating Heng's strategy for unknowns and silence
2) Creating models for MFCC and raw wave features
	- I will have to spin up 3 servers so that I can run these in parallel
	- I will have to adopt argparsing so that I don't have to keep changing things, and can keep GH up-to-date
3) Ensembling these models together
	- Different models to try:
		1) Models with no noise
		1) Models using mixup data augmentation
		1) Models that train on pseudo labels for silence and unknowns
		1) 5 Overdrive Pseudo-Label models with different seeds, since this is my best model
		2) Ok Conv
		3) An RNN version of Overdrive
		4) An average-pooled (at all levels) version of Overdrive
		4) An Overdrive that squeezes down the feature set with extrac convs
		4) A flat (or locally max-pooled) version of Overdrive
		5) Overdrive models that were trained concurrently with the known unknowns
		6) New Drive
		7) Models that do volume equalization differently (avg vol, peak vol with larger strides)
		8) Models that use frame-by-frame standardization
		8) Heng's MFCC models, 2 of them
		9) Heng's Raw Wave models, 2 of them
		10) Heng's Log Mel models, 2 of them
		11) The Hello Edge DS-CNN
		12) The Hello Edge R-CNN
	
		

